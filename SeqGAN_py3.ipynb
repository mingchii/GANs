{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SeqGAN_py3.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mingchii/GANs-on-limit-order-book/blob/master/SeqGAN_py3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DwzeWG4oQHvS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "class Gen_Data_loader():\n",
        "    def __init__(self, batch_size):\n",
        "        self.batch_size = batch_size\n",
        "        self.token_stream = []\n",
        "\n",
        "    def create_batches(self, data_file):\n",
        "        self.token_stream = []\n",
        "        with open(data_file, 'r') as f:\n",
        "            for line in f:\n",
        "                line = line.strip()\n",
        "                line = line.split()\n",
        "                parse_line = [int(x) for x in line]\n",
        "                if len(parse_line) == 20:\n",
        "                    self.token_stream.append(parse_line)\n",
        "\n",
        "        self.num_batch = int(len(self.token_stream) / self.batch_size)\n",
        "        self.token_stream = self.token_stream[:self.num_batch * self.batch_size]\n",
        "        self.sequence_batch = np.split(np.array(self.token_stream), self.num_batch, 0)\n",
        "        self.pointer = 0\n",
        "\n",
        "    def next_batch(self):\n",
        "        ret = self.sequence_batch[self.pointer]\n",
        "        self.pointer = (self.pointer + 1) % self.num_batch\n",
        "        return ret\n",
        "\n",
        "    def reset_pointer(self):\n",
        "        self.pointer = 0\n",
        "\n",
        "\n",
        "class Dis_dataloader():\n",
        "    def __init__(self, batch_size):\n",
        "        self.batch_size = batch_size\n",
        "        self.sentences = np.array([])\n",
        "        self.labels = np.array([])\n",
        "\n",
        "    def load_train_data(self, positive_file, negative_file):\n",
        "        # Load data\n",
        "        positive_examples = []\n",
        "        negative_examples = []\n",
        "        with open(positive_file)as fin:\n",
        "            for line in fin:\n",
        "                line = line.strip()\n",
        "                line = line.split()\n",
        "                parse_line = [int(x) for x in line]\n",
        "                positive_examples.append(parse_line)\n",
        "        with open(negative_file)as fin:\n",
        "            for line in fin:\n",
        "                line = line.strip()\n",
        "                line = line.split()\n",
        "                parse_line = [int(x) for x in line]\n",
        "                if len(parse_line) == 20:\n",
        "                    negative_examples.append(parse_line)\n",
        "        self.sentences = np.array(positive_examples + negative_examples)\n",
        "\n",
        "        # Generate labels\n",
        "        positive_labels = [[0, 1] for _ in positive_examples]\n",
        "        negative_labels = [[1, 0] for _ in negative_examples]\n",
        "        self.labels = np.concatenate([positive_labels, negative_labels], 0)\n",
        "\n",
        "        # Shuffle the data\n",
        "        shuffle_indices = np.random.permutation(np.arange(len(self.labels)))\n",
        "        self.sentences = self.sentences[shuffle_indices]\n",
        "        self.labels = self.labels[shuffle_indices]\n",
        "\n",
        "        # Split batches\n",
        "        self.num_batch = int(len(self.labels) / self.batch_size)\n",
        "        self.sentences = self.sentences[:self.num_batch * self.batch_size]\n",
        "        self.labels = self.labels[:self.num_batch * self.batch_size]\n",
        "        self.sentences_batches = np.split(self.sentences, self.num_batch, 0)\n",
        "        self.labels_batches = np.split(self.labels, self.num_batch, 0)\n",
        "\n",
        "        self.pointer = 0\n",
        "\n",
        "\n",
        "    def next_batch(self):\n",
        "        ret = self.sentences_batches[self.pointer], self.labels_batches[self.pointer]\n",
        "        self.pointer = (self.pointer + 1) % self.num_batch\n",
        "        return ret\n",
        "\n",
        "    def reset_pointer(self):\n",
        "        self.pointer = 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1WL-lllYD7mS",
        "colab_type": "code",
        "outputId": "6c035aa3-111f-47ec-e8e8-c70670bcb960",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 63
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.python.ops import tensor_array_ops, control_flow_ops\n",
        "\n",
        "\n",
        "class Generator(object):\n",
        "    def __init__(self, num_emb, batch_size, emb_dim, hidden_dim,\n",
        "                 sequence_length, start_token,\n",
        "                 learning_rate=0.01, reward_gamma=0.95):\n",
        "        self.num_emb = num_emb\n",
        "        self.batch_size = batch_size\n",
        "        self.emb_dim = emb_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.sequence_length = sequence_length\n",
        "        self.start_token = tf.constant([start_token] * self.batch_size, dtype=tf.int32)\n",
        "        self.learning_rate = tf.Variable(float(learning_rate), trainable=False)\n",
        "        self.reward_gamma = reward_gamma\n",
        "        self.g_params = []\n",
        "        self.d_params = []\n",
        "        self.temperature = 1.0\n",
        "        self.grad_clip = 5.0\n",
        "\n",
        "        self.expected_reward = tf.Variable(tf.zeros([self.sequence_length]))\n",
        "\n",
        "        with tf.variable_scope('generator'):\n",
        "            self.g_embeddings = tf.Variable(self.init_matrix([self.num_emb, self.emb_dim]))\n",
        "            self.g_params.append(self.g_embeddings)\n",
        "            self.g_recurrent_unit = self.create_recurrent_unit(self.g_params)  # maps h_tm1 to h_t for generator\n",
        "            self.g_output_unit = self.create_output_unit(self.g_params)  # maps h_t to o_t (output token logits)\n",
        "\n",
        "        # placeholder definition\n",
        "        self.x = tf.placeholder(tf.int32, shape=[self.batch_size, self.sequence_length]) # sequence of tokens generated by generator\n",
        "        self.rewards = tf.placeholder(tf.float32, shape=[self.batch_size, self.sequence_length]) # get from rollout policy and discriminator\n",
        "\n",
        "        # processed for batch\n",
        "        with tf.device(\"/cpu:0\"):\n",
        "            self.processed_x = tf.transpose(tf.nn.embedding_lookup(self.g_embeddings, self.x), perm=[1, 0, 2])  # seq_length x batch_size x emb_dim\n",
        "\n",
        "        # Initial states\n",
        "        self.h0 = tf.zeros([self.batch_size, self.hidden_dim])\n",
        "        self.h0 = tf.stack([self.h0, self.h0])\n",
        "\n",
        "        gen_o = tensor_array_ops.TensorArray(dtype=tf.float32, size=self.sequence_length,\n",
        "                                             dynamic_size=False, infer_shape=True)\n",
        "        gen_x = tensor_array_ops.TensorArray(dtype=tf.int32, size=self.sequence_length,\n",
        "                                             dynamic_size=False, infer_shape=True)\n",
        "\n",
        "        def _g_recurrence(i, x_t, h_tm1, gen_o, gen_x):\n",
        "            h_t = self.g_recurrent_unit(x_t, h_tm1)  # hidden_memory_tuple\n",
        "            o_t = self.g_output_unit(h_t)  # batch x vocab , logits not prob\n",
        "            log_prob = tf.log(tf.nn.softmax(o_t))\n",
        "            next_token = tf.cast(tf.reshape(tf.multinomial(log_prob, 1), [self.batch_size]), tf.int32)\n",
        "            x_tp1 = tf.nn.embedding_lookup(self.g_embeddings, next_token)  # batch x emb_dim\n",
        "            gen_o = gen_o.write(i, tf.reduce_sum(tf.multiply(tf.one_hot(next_token, self.num_emb, 1.0, 0.0),\n",
        "                                                             tf.nn.softmax(o_t)), 1))  # [batch_size] , prob\n",
        "            gen_x = gen_x.write(i, next_token)  # indices, batch_size\n",
        "            return i + 1, x_tp1, h_t, gen_o, gen_x\n",
        "\n",
        "        _, _, _, self.gen_o, self.gen_x = control_flow_ops.while_loop(\n",
        "            cond=lambda i, _1, _2, _3, _4: i < self.sequence_length,\n",
        "            body=_g_recurrence,\n",
        "            loop_vars=(tf.constant(0, dtype=tf.int32),\n",
        "                       tf.nn.embedding_lookup(self.g_embeddings, self.start_token), self.h0, gen_o, gen_x))\n",
        "\n",
        "        self.gen_x = self.gen_x.stack()  # seq_length x batch_size\n",
        "        self.gen_x = tf.transpose(self.gen_x, perm=[1, 0])  # batch_size x seq_length\n",
        "\n",
        "        # supervised pretraining for generator\n",
        "        g_predictions = tensor_array_ops.TensorArray(\n",
        "            dtype=tf.float32, size=self.sequence_length,\n",
        "            dynamic_size=False, infer_shape=True)\n",
        "\n",
        "        ta_emb_x = tensor_array_ops.TensorArray(\n",
        "            dtype=tf.float32, size=self.sequence_length)\n",
        "        ta_emb_x = ta_emb_x.unstack(self.processed_x)\n",
        "\n",
        "        def _pretrain_recurrence(i, x_t, h_tm1, g_predictions):\n",
        "            h_t = self.g_recurrent_unit(x_t, h_tm1)\n",
        "            o_t = self.g_output_unit(h_t)\n",
        "            g_predictions = g_predictions.write(i, tf.nn.softmax(o_t))  # batch x vocab_size\n",
        "            x_tp1 = ta_emb_x.read(i)\n",
        "            return i + 1, x_tp1, h_t, g_predictions\n",
        "\n",
        "        _, _, _, self.g_predictions = control_flow_ops.while_loop(\n",
        "            cond=lambda i, _1, _2, _3: i < self.sequence_length,\n",
        "            body=_pretrain_recurrence,\n",
        "            loop_vars=(tf.constant(0, dtype=tf.int32),\n",
        "                       tf.nn.embedding_lookup(self.g_embeddings, self.start_token),\n",
        "                       self.h0, g_predictions))\n",
        "\n",
        "        self.g_predictions = tf.transpose(self.g_predictions.stack(), perm=[1, 0, 2])  # batch_size x seq_length x vocab_size\n",
        "\n",
        "        # pretraining loss\n",
        "        self.pretrain_loss = -tf.reduce_sum(\n",
        "            tf.one_hot(tf.to_int32(tf.reshape(self.x, [-1])), self.num_emb, 1.0, 0.0) * tf.log(\n",
        "                tf.clip_by_value(tf.reshape(self.g_predictions, [-1, self.num_emb]), 1e-20, 1.0)\n",
        "            )\n",
        "        ) / (self.sequence_length * self.batch_size)\n",
        "\n",
        "        # training updates\n",
        "        pretrain_opt = self.g_optimizer(self.learning_rate)\n",
        "\n",
        "        self.pretrain_grad, _ = tf.clip_by_global_norm(tf.gradients(self.pretrain_loss, self.g_params), self.grad_clip)\n",
        "        self.pretrain_updates = pretrain_opt.apply_gradients(zip(self.pretrain_grad, self.g_params))\n",
        "\n",
        "        #######################################################################################################\n",
        "        #  Unsupervised Training\n",
        "        #######################################################################################################\n",
        "        self.g_loss = -tf.reduce_sum(\n",
        "            tf.reduce_sum(\n",
        "                tf.one_hot(tf.to_int32(tf.reshape(self.x, [-1])), self.num_emb, 1.0, 0.0) * tf.log(\n",
        "                    tf.clip_by_value(tf.reshape(self.g_predictions, [-1, self.num_emb]), 1e-20, 1.0)\n",
        "                ), 1) * tf.reshape(self.rewards, [-1])\n",
        "        )\n",
        "\n",
        "        g_opt = self.g_optimizer(self.learning_rate)\n",
        "\n",
        "        self.g_grad, _ = tf.clip_by_global_norm(tf.gradients(self.g_loss, self.g_params), self.grad_clip)\n",
        "        self.g_updates = g_opt.apply_gradients(zip(self.g_grad, self.g_params))\n",
        "\n",
        "    def generate(self, sess):\n",
        "        outputs = sess.run(self.gen_x)\n",
        "        return outputs\n",
        "\n",
        "    def pretrain_step(self, sess, x):\n",
        "        outputs = sess.run([self.pretrain_updates, self.pretrain_loss], feed_dict={self.x: x})\n",
        "        return outputs\n",
        "\n",
        "    def init_matrix(self, shape):\n",
        "        return tf.random_normal(shape, stddev=0.1)\n",
        "\n",
        "    def init_vector(self, shape):\n",
        "        return tf.zeros(shape)\n",
        "\n",
        "    def create_recurrent_unit(self, params):\n",
        "        # Weights and Bias for input and hidden tensor\n",
        "        self.Wi = tf.Variable(self.init_matrix([self.emb_dim, self.hidden_dim]))\n",
        "        self.Ui = tf.Variable(self.init_matrix([self.hidden_dim, self.hidden_dim]))\n",
        "        self.bi = tf.Variable(self.init_matrix([self.hidden_dim]))\n",
        "\n",
        "        self.Wf = tf.Variable(self.init_matrix([self.emb_dim, self.hidden_dim]))\n",
        "        self.Uf = tf.Variable(self.init_matrix([self.hidden_dim, self.hidden_dim]))\n",
        "        self.bf = tf.Variable(self.init_matrix([self.hidden_dim]))\n",
        "\n",
        "        self.Wog = tf.Variable(self.init_matrix([self.emb_dim, self.hidden_dim]))\n",
        "        self.Uog = tf.Variable(self.init_matrix([self.hidden_dim, self.hidden_dim]))\n",
        "        self.bog = tf.Variable(self.init_matrix([self.hidden_dim]))\n",
        "\n",
        "        self.Wc = tf.Variable(self.init_matrix([self.emb_dim, self.hidden_dim]))\n",
        "        self.Uc = tf.Variable(self.init_matrix([self.hidden_dim, self.hidden_dim]))\n",
        "        self.bc = tf.Variable(self.init_matrix([self.hidden_dim]))\n",
        "        params.extend([\n",
        "            self.Wi, self.Ui, self.bi,\n",
        "            self.Wf, self.Uf, self.bf,\n",
        "            self.Wog, self.Uog, self.bog,\n",
        "            self.Wc, self.Uc, self.bc])\n",
        "\n",
        "        def unit(x, hidden_memory_tm1):\n",
        "            previous_hidden_state, c_prev = tf.unstack(hidden_memory_tm1)\n",
        "\n",
        "            # Input Gate\n",
        "            i = tf.sigmoid(\n",
        "                tf.matmul(x, self.Wi) +\n",
        "                tf.matmul(previous_hidden_state, self.Ui) + self.bi\n",
        "            )\n",
        "\n",
        "            # Forget Gate\n",
        "            f = tf.sigmoid(\n",
        "                tf.matmul(x, self.Wf) +\n",
        "                tf.matmul(previous_hidden_state, self.Uf) + self.bf\n",
        "            )\n",
        "\n",
        "            # Output Gate\n",
        "            o = tf.sigmoid(\n",
        "                tf.matmul(x, self.Wog) +\n",
        "                tf.matmul(previous_hidden_state, self.Uog) + self.bog\n",
        "            )\n",
        "\n",
        "            # New Memory Cell\n",
        "            c_ = tf.nn.tanh(\n",
        "                tf.matmul(x, self.Wc) +\n",
        "                tf.matmul(previous_hidden_state, self.Uc) + self.bc\n",
        "            )\n",
        "\n",
        "            # Final Memory cell\n",
        "            c = f * c_prev + i * c_\n",
        "\n",
        "            # Current Hidden state\n",
        "            current_hidden_state = o * tf.nn.tanh(c)\n",
        "\n",
        "            return tf.stack([current_hidden_state, c])\n",
        "\n",
        "        return unit\n",
        "\n",
        "    def create_output_unit(self, params):\n",
        "        self.Wo = tf.Variable(self.init_matrix([self.hidden_dim, self.num_emb]))\n",
        "        self.bo = tf.Variable(self.init_matrix([self.num_emb]))\n",
        "        params.extend([self.Wo, self.bo])\n",
        "\n",
        "        def unit(hidden_memory_tuple):\n",
        "            hidden_state, c_prev = tf.unstack(hidden_memory_tuple)\n",
        "            # hidden_state : batch x hidden_dim\n",
        "            logits = tf.matmul(hidden_state, self.Wo) + self.bo\n",
        "            # output = tf.nn.softmax(logits)\n",
        "            return logits\n",
        "\n",
        "        return unit\n",
        "\n",
        "    def g_optimizer(self, *args, **kwargs):\n",
        "        return tf.train.AdamOptimizer(*args, **kwargs)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dgNXG3rzP1oB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "# An alternative to tf.nn.rnn_cell._linear function, which has been removed in Tensorfow 1.0.1\n",
        "# The highway layer is borrowed from https://github.com/mkroutikov/tf-lstm-char-cnn\n",
        "def linear(input_, output_size, scope=None):\n",
        "    '''\n",
        "    Linear map: output[k] = sum_i(Matrix[k, i] * input_[i] ) + Bias[k]\n",
        "    Args:\n",
        "    input_: a tensor or a list of 2D, batch x n, Tensors.\n",
        "    output_size: int, second dimension of W[i].\n",
        "    scope: VariableScope for the created subgraph; defaults to \"Linear\".\n",
        "  Returns:\n",
        "    A 2D Tensor with shape [batch x output_size] equal to\n",
        "    sum_i(input_[i] * W[i]), where W[i]s are newly created matrices.\n",
        "  Raises:\n",
        "    ValueError: if some of the arguments has unspecified or wrong shape.\n",
        "  '''\n",
        "\n",
        "    shape = input_.get_shape().as_list()\n",
        "    if len(shape) != 2:\n",
        "        raise ValueError(\"Linear is expecting 2D arguments: %s\" % str(shape))\n",
        "    if not shape[1]:\n",
        "        raise ValueError(\"Linear expects shape[1] of arguments: %s\" % str(shape))\n",
        "    input_size = shape[1]\n",
        "\n",
        "    # Now the computation.\n",
        "    with tf.variable_scope(scope or \"SimpleLinear\"):\n",
        "        matrix = tf.get_variable(\"Matrix\", [output_size, input_size], dtype=input_.dtype)\n",
        "        bias_term = tf.get_variable(\"Bias\", [output_size], dtype=input_.dtype)\n",
        "\n",
        "    return tf.matmul(input_, tf.transpose(matrix)) + bias_term\n",
        "\n",
        "def highway(input_, size, num_layers=1, bias=-2.0, f=tf.nn.relu, scope='Highway'):\n",
        "    \"\"\"Highway Network (cf. http://arxiv.org/abs/1505.00387).\n",
        "    t = sigmoid(Wy + b)\n",
        "    z = t * g(Wy + b) + (1 - t) * y\n",
        "    where g is nonlinearity, t is transform gate, and (1 - t) is carry gate.\n",
        "    \"\"\"\n",
        "\n",
        "    with tf.variable_scope(scope):\n",
        "        for idx in range(num_layers):\n",
        "            g = f(linear(input_, size, scope='highway_lin_%d' % idx))\n",
        "\n",
        "            t = tf.sigmoid(linear(input_, size, scope='highway_gate_%d' % idx) + bias)\n",
        "\n",
        "            output = t * g + (1. - t) * input_\n",
        "            input_ = output\n",
        "\n",
        "    return output\n",
        "\n",
        "class Discriminator(object):\n",
        "    \"\"\"\n",
        "    A CNN for text classification.\n",
        "    Uses an embedding layer, followed by a convolutional, max-pooling and softmax layer.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "            self, sequence_length, num_classes, vocab_size,\n",
        "            embedding_size, filter_sizes, num_filters, l2_reg_lambda=0.0):\n",
        "        # Placeholders for input, output and dropout\n",
        "        self.input_x = tf.placeholder(tf.int32, [None, sequence_length], name=\"input_x\")\n",
        "        self.input_y = tf.placeholder(tf.float32, [None, num_classes], name=\"input_y\")\n",
        "        self.dropout_keep_prob = tf.placeholder(tf.float32, name=\"dropout_keep_prob\")\n",
        "\n",
        "        # Keeping track of l2 regularization loss (optional)\n",
        "        l2_loss = tf.constant(0.0)\n",
        "        \n",
        "        with tf.variable_scope('discriminator'):\n",
        "\n",
        "            # Embedding layer\n",
        "            with tf.device('/cpu:0'), tf.name_scope(\"embedding\"):\n",
        "                self.W = tf.Variable(\n",
        "                    tf.random_uniform([vocab_size, embedding_size], -1.0, 1.0),\n",
        "                    name=\"W\")\n",
        "                self.embedded_chars = tf.nn.embedding_lookup(self.W, self.input_x)\n",
        "                self.embedded_chars_expanded = tf.expand_dims(self.embedded_chars, -1)\n",
        "\n",
        "            # Create a convolution + maxpool layer for each filter size\n",
        "            pooled_outputs = []\n",
        "            for filter_size, num_filter in zip(filter_sizes, num_filters):\n",
        "                with tf.name_scope(\"conv-maxpool-%s\" % filter_size):\n",
        "                    # Convolution Layer\n",
        "                    filter_shape = [filter_size, embedding_size, 1, num_filter]\n",
        "                    W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=\"W\")\n",
        "                    b = tf.Variable(tf.constant(0.1, shape=[num_filter]), name=\"b\")\n",
        "                    conv = tf.nn.conv2d(\n",
        "                        self.embedded_chars_expanded,\n",
        "                        W,\n",
        "                        strides=[1, 1, 1, 1],\n",
        "                        padding=\"VALID\",\n",
        "                        name=\"conv\")\n",
        "                    # Apply nonlinearity\n",
        "                    h = tf.nn.relu(tf.nn.bias_add(conv, b), name=\"relu\")\n",
        "                    # Maxpooling over the outputs\n",
        "                    pooled = tf.nn.max_pool(\n",
        "                        h,\n",
        "                        ksize=[1, sequence_length - filter_size + 1, 1, 1],\n",
        "                        strides=[1, 1, 1, 1],\n",
        "                        padding='VALID',\n",
        "                        name=\"pool\")\n",
        "                    pooled_outputs.append(pooled)\n",
        "            \n",
        "            # Combine all the pooled features\n",
        "            num_filters_total = sum(num_filters)\n",
        "            self.h_pool = tf.concat(pooled_outputs, 3)\n",
        "            self.h_pool_flat = tf.reshape(self.h_pool, [-1, num_filters_total])\n",
        "\n",
        "            # Add highway\n",
        "            with tf.name_scope(\"highway\"):\n",
        "                self.h_highway = highway(self.h_pool_flat, self.h_pool_flat.get_shape()[1], 1, 0)\n",
        "\n",
        "            # Add dropout\n",
        "            with tf.name_scope(\"dropout\"):\n",
        "                self.h_drop = tf.nn.dropout(self.h_highway, self.dropout_keep_prob)\n",
        "\n",
        "            # Final (unnormalized) scores and predictions\n",
        "            with tf.name_scope(\"output\"):\n",
        "                W = tf.Variable(tf.truncated_normal([num_filters_total, num_classes], stddev=0.1), name=\"W\")\n",
        "                b = tf.Variable(tf.constant(0.1, shape=[num_classes]), name=\"b\")\n",
        "                l2_loss += tf.nn.l2_loss(W)\n",
        "                l2_loss += tf.nn.l2_loss(b)\n",
        "                self.scores = tf.nn.xw_plus_b(self.h_drop, W, b, name=\"scores\")\n",
        "                self.ypred_for_auc = tf.nn.softmax(self.scores)\n",
        "                self.predictions = tf.argmax(self.scores, 1, name=\"predictions\")\n",
        "\n",
        "            # CalculateMean cross-entropy loss\n",
        "            with tf.name_scope(\"loss\"):\n",
        "                losses = tf.nn.softmax_cross_entropy_with_logits(logits=self.scores, labels=self.input_y)\n",
        "                self.loss = tf.reduce_mean(losses) + l2_reg_lambda * l2_loss\n",
        "\n",
        "        self.params = [param for param in tf.trainable_variables() if 'discriminator' in param.name]\n",
        "        d_optimizer = tf.train.AdamOptimizer(1e-4)\n",
        "        grads_and_vars = d_optimizer.compute_gradients(self.loss, self.params, aggregation_method=2)\n",
        "        self.train_op = d_optimizer.apply_gradients(grads_and_vars)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uKQQGAUuQNzs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.python.ops import tensor_array_ops, control_flow_ops\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class ROLLOUT(object):\n",
        "    def __init__(self, lstm, update_rate):\n",
        "        self.lstm = lstm\n",
        "        self.update_rate = update_rate\n",
        "\n",
        "        self.num_emb = self.lstm.num_emb\n",
        "        self.batch_size = self.lstm.batch_size\n",
        "        self.emb_dim = self.lstm.emb_dim\n",
        "        self.hidden_dim = self.lstm.hidden_dim\n",
        "        self.sequence_length = self.lstm.sequence_length\n",
        "        self.start_token = tf.identity(self.lstm.start_token)\n",
        "        self.learning_rate = self.lstm.learning_rate\n",
        "\n",
        "        self.g_embeddings = tf.identity(self.lstm.g_embeddings)\n",
        "        self.g_recurrent_unit = self.create_recurrent_unit()  # maps h_tm1 to h_t for generator\n",
        "        self.g_output_unit = self.create_output_unit()  # maps h_t to o_t (output token logits)\n",
        "\n",
        "        #####################################################################################################\n",
        "        # placeholder definition\n",
        "        self.x = tf.placeholder(tf.int32, shape=[self.batch_size, self.sequence_length]) # sequence of tokens generated by generator\n",
        "        self.given_num = tf.placeholder(tf.int32)\n",
        "\n",
        "        # processed for batch\n",
        "        with tf.device(\"/cpu:0\"):\n",
        "            self.processed_x = tf.transpose(tf.nn.embedding_lookup(self.g_embeddings, self.x), perm=[1, 0, 2])  # seq_length x batch_size x emb_dim\n",
        "\n",
        "        ta_emb_x = tensor_array_ops.TensorArray(\n",
        "            dtype=tf.float32, size=self.sequence_length)\n",
        "        ta_emb_x = ta_emb_x.unstack(self.processed_x)\n",
        "\n",
        "        ta_x = tensor_array_ops.TensorArray(dtype=tf.int32, size=self.sequence_length)\n",
        "        ta_x = ta_x.unstack(tf.transpose(self.x, perm=[1, 0]))\n",
        "        #####################################################################################################\n",
        "\n",
        "        self.h0 = tf.zeros([self.batch_size, self.hidden_dim])\n",
        "        self.h0 = tf.stack([self.h0, self.h0])\n",
        "\n",
        "        gen_x = tensor_array_ops.TensorArray(dtype=tf.int32, size=self.sequence_length,\n",
        "                                             dynamic_size=False, infer_shape=True)\n",
        "\n",
        "        # When current index i < given_num, use the provided tokens as the input at each time step\n",
        "        def _g_recurrence_1(i, x_t, h_tm1, given_num, gen_x):\n",
        "            h_t = self.g_recurrent_unit(x_t, h_tm1)  # hidden_memory_tuple\n",
        "            x_tp1 = ta_emb_x.read(i)\n",
        "            gen_x = gen_x.write(i, ta_x.read(i))\n",
        "            return i + 1, x_tp1, h_t, given_num, gen_x\n",
        "\n",
        "        # When current index i >= given_num, start roll-out, use the output as time step t as the input at time step t+1\n",
        "        def _g_recurrence_2(i, x_t, h_tm1, given_num, gen_x):\n",
        "            h_t = self.g_recurrent_unit(x_t, h_tm1)  # hidden_memory_tuple\n",
        "            o_t = self.g_output_unit(h_t)  # batch x vocab , logits not prob\n",
        "            log_prob = tf.log(tf.nn.softmax(o_t))\n",
        "            next_token = tf.cast(tf.reshape(tf.multinomial(log_prob, 1), [self.batch_size]), tf.int32)\n",
        "            x_tp1 = tf.nn.embedding_lookup(self.g_embeddings, next_token)  # batch x emb_dim\n",
        "            gen_x = gen_x.write(i, next_token)  # indices, batch_size\n",
        "            return i + 1, x_tp1, h_t, given_num, gen_x\n",
        "\n",
        "        i, x_t, h_tm1, given_num, self.gen_x = control_flow_ops.while_loop(\n",
        "            cond=lambda i, _1, _2, given_num, _4: i < given_num,\n",
        "            body=_g_recurrence_1,\n",
        "            loop_vars=(tf.constant(0, dtype=tf.int32),\n",
        "                       tf.nn.embedding_lookup(self.g_embeddings, self.start_token), self.h0, self.given_num, gen_x))\n",
        "\n",
        "        _, _, _, _, self.gen_x = control_flow_ops.while_loop(\n",
        "            cond=lambda i, _1, _2, _3, _4: i < self.sequence_length,\n",
        "            body=_g_recurrence_2,\n",
        "            loop_vars=(i, x_t, h_tm1, given_num, self.gen_x))\n",
        "\n",
        "        self.gen_x = self.gen_x.stack()  # seq_length x batch_size\n",
        "        self.gen_x = tf.transpose(self.gen_x, perm=[1, 0])  # batch_size x seq_length\n",
        "\n",
        "    def get_reward(self, sess, input_x, rollout_num, discriminator):\n",
        "        rewards = []\n",
        "        for i in range(rollout_num):\n",
        "            # given_num between 1 to sequence_length - 1 for a part completed sentence\n",
        "            for given_num in range(1, self.sequence_length ):\n",
        "                feed = {self.x: input_x, self.given_num: given_num}\n",
        "                samples = sess.run(self.gen_x, feed)\n",
        "                feed = {discriminator.input_x: samples, discriminator.dropout_keep_prob: 1.0}\n",
        "                ypred_for_auc = sess.run(discriminator.ypred_for_auc, feed)\n",
        "                ypred = np.array([item[1] for item in ypred_for_auc])\n",
        "                if i == 0:\n",
        "                    rewards.append(ypred)\n",
        "                else:\n",
        "                    rewards[given_num - 1] += ypred\n",
        "\n",
        "            # the last token reward\n",
        "            feed = {discriminator.input_x: input_x, discriminator.dropout_keep_prob: 1.0}\n",
        "            ypred_for_auc = sess.run(discriminator.ypred_for_auc, feed)\n",
        "            ypred = np.array([item[1] for item in ypred_for_auc])\n",
        "            if i == 0:\n",
        "                rewards.append(ypred)\n",
        "            else:\n",
        "                # completed sentence reward\n",
        "                rewards[self.sequence_length - 1] += ypred\n",
        "\n",
        "        rewards = np.transpose(np.array(rewards)) / (1.0 * rollout_num)  # batch_size x seq_length\n",
        "        return rewards\n",
        "\n",
        "    def create_recurrent_unit(self):\n",
        "        # Weights and Bias for input and hidden tensor\n",
        "        self.Wi = tf.identity(self.lstm.Wi)\n",
        "        self.Ui = tf.identity(self.lstm.Ui)\n",
        "        self.bi = tf.identity(self.lstm.bi)\n",
        "\n",
        "        self.Wf = tf.identity(self.lstm.Wf)\n",
        "        self.Uf = tf.identity(self.lstm.Uf)\n",
        "        self.bf = tf.identity(self.lstm.bf)\n",
        "\n",
        "        self.Wog = tf.identity(self.lstm.Wog)\n",
        "        self.Uog = tf.identity(self.lstm.Uog)\n",
        "        self.bog = tf.identity(self.lstm.bog)\n",
        "\n",
        "        self.Wc = tf.identity(self.lstm.Wc)\n",
        "        self.Uc = tf.identity(self.lstm.Uc)\n",
        "        self.bc = tf.identity(self.lstm.bc)\n",
        "\n",
        "        def unit(x, hidden_memory_tm1):\n",
        "            previous_hidden_state, c_prev = tf.unstack(hidden_memory_tm1)\n",
        "\n",
        "            # Input Gate\n",
        "            i = tf.sigmoid(\n",
        "                tf.matmul(x, self.Wi) +\n",
        "                tf.matmul(previous_hidden_state, self.Ui) + self.bi\n",
        "            )\n",
        "\n",
        "            # Forget Gate\n",
        "            f = tf.sigmoid(\n",
        "                tf.matmul(x, self.Wf) +\n",
        "                tf.matmul(previous_hidden_state, self.Uf) + self.bf\n",
        "            )\n",
        "\n",
        "            # Output Gate\n",
        "            o = tf.sigmoid(\n",
        "                tf.matmul(x, self.Wog) +\n",
        "                tf.matmul(previous_hidden_state, self.Uog) + self.bog\n",
        "            )\n",
        "\n",
        "            # New Memory Cell\n",
        "            c_ = tf.nn.tanh(\n",
        "                tf.matmul(x, self.Wc) +\n",
        "                tf.matmul(previous_hidden_state, self.Uc) + self.bc\n",
        "            )\n",
        "\n",
        "            # Final Memory cell\n",
        "            c = f * c_prev + i * c_\n",
        "\n",
        "            # Current Hidden state\n",
        "            current_hidden_state = o * tf.nn.tanh(c)\n",
        "\n",
        "            return tf.stack([current_hidden_state, c])\n",
        "\n",
        "        return unit\n",
        "\n",
        "    def update_recurrent_unit(self):\n",
        "        # Weights and Bias for input and hidden tensor\n",
        "        self.Wi = self.update_rate * self.Wi + (1 - self.update_rate) * tf.identity(self.lstm.Wi)\n",
        "        self.Ui = self.update_rate * self.Ui + (1 - self.update_rate) * tf.identity(self.lstm.Ui)\n",
        "        self.bi = self.update_rate * self.bi + (1 - self.update_rate) * tf.identity(self.lstm.bi)\n",
        "\n",
        "        self.Wf = self.update_rate * self.Wf + (1 - self.update_rate) * tf.identity(self.lstm.Wf)\n",
        "        self.Uf = self.update_rate * self.Uf + (1 - self.update_rate) * tf.identity(self.lstm.Uf)\n",
        "        self.bf = self.update_rate * self.bf + (1 - self.update_rate) * tf.identity(self.lstm.bf)\n",
        "\n",
        "        self.Wog = self.update_rate * self.Wog + (1 - self.update_rate) * tf.identity(self.lstm.Wog)\n",
        "        self.Uog = self.update_rate * self.Uog + (1 - self.update_rate) * tf.identity(self.lstm.Uog)\n",
        "        self.bog = self.update_rate * self.bog + (1 - self.update_rate) * tf.identity(self.lstm.bog)\n",
        "\n",
        "        self.Wc = self.update_rate * self.Wc + (1 - self.update_rate) * tf.identity(self.lstm.Wc)\n",
        "        self.Uc = self.update_rate * self.Uc + (1 - self.update_rate) * tf.identity(self.lstm.Uc)\n",
        "        self.bc = self.update_rate * self.bc + (1 - self.update_rate) * tf.identity(self.lstm.bc)\n",
        "\n",
        "        def unit(x, hidden_memory_tm1):\n",
        "            previous_hidden_state, c_prev = tf.unstack(hidden_memory_tm1)\n",
        "\n",
        "            # Input Gate\n",
        "            i = tf.sigmoid(\n",
        "                tf.matmul(x, self.Wi) +\n",
        "                tf.matmul(previous_hidden_state, self.Ui) + self.bi\n",
        "            )\n",
        "\n",
        "            # Forget Gate\n",
        "            f = tf.sigmoid(\n",
        "                tf.matmul(x, self.Wf) +\n",
        "                tf.matmul(previous_hidden_state, self.Uf) + self.bf\n",
        "            )\n",
        "\n",
        "            # Output Gate\n",
        "            o = tf.sigmoid(\n",
        "                tf.matmul(x, self.Wog) +\n",
        "                tf.matmul(previous_hidden_state, self.Uog) + self.bog\n",
        "            )\n",
        "\n",
        "            # New Memory Cell\n",
        "            c_ = tf.nn.tanh(\n",
        "                tf.matmul(x, self.Wc) +\n",
        "                tf.matmul(previous_hidden_state, self.Uc) + self.bc\n",
        "            )\n",
        "\n",
        "            # Final Memory cell\n",
        "            c = f * c_prev + i * c_\n",
        "\n",
        "            # Current Hidden state\n",
        "            current_hidden_state = o * tf.nn.tanh(c)\n",
        "\n",
        "            return tf.stack([current_hidden_state, c])\n",
        "\n",
        "        return unit\n",
        "\n",
        "    def create_output_unit(self):\n",
        "        self.Wo = tf.identity(self.lstm.Wo)\n",
        "        self.bo = tf.identity(self.lstm.bo)\n",
        "\n",
        "        def unit(hidden_memory_tuple):\n",
        "            hidden_state, c_prev = tf.unstack(hidden_memory_tuple)\n",
        "            # hidden_state : batch x hidden_dim\n",
        "            logits = tf.matmul(hidden_state, self.Wo) + self.bo\n",
        "            # output = tf.nn.softmax(logits)\n",
        "            return logits\n",
        "\n",
        "        return unit\n",
        "\n",
        "    def update_output_unit(self):\n",
        "        self.Wo = self.update_rate * self.Wo + (1 - self.update_rate) * tf.identity(self.lstm.Wo)\n",
        "        self.bo = self.update_rate * self.bo + (1 - self.update_rate) * tf.identity(self.lstm.bo)\n",
        "\n",
        "        def unit(hidden_memory_tuple):\n",
        "            hidden_state, c_prev = tf.unstack(hidden_memory_tuple)\n",
        "            # hidden_state : batch x hidden_dim\n",
        "            logits = tf.matmul(hidden_state, self.Wo) + self.bo\n",
        "            # output = tf.nn.softmax(logits)\n",
        "            return logits\n",
        "\n",
        "        return unit\n",
        "\n",
        "    def update_params(self):\n",
        "        self.g_embeddings = tf.identity(self.lstm.g_embeddings)\n",
        "        self.g_recurrent_unit = self.update_recurrent_unit()\n",
        "        self.g_output_unit = self.update_output_unit()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Rp5654pQTan",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.python.ops import tensor_array_ops, control_flow_ops\n",
        "\n",
        "\n",
        "class TARGET_LSTM(object):\n",
        "    def __init__(self, num_emb, batch_size, emb_dim, hidden_dim, sequence_length, start_token, params):\n",
        "        self.num_emb = num_emb\n",
        "        self.batch_size = batch_size\n",
        "        self.emb_dim = emb_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.sequence_length = sequence_length\n",
        "        self.start_token = tf.constant([start_token] * self.batch_size, dtype=tf.int32)\n",
        "        self.g_params = []\n",
        "        self.temperature = 1.0\n",
        "        self.params = params\n",
        "\n",
        "        tf.set_random_seed(66)\n",
        "\n",
        "        with tf.variable_scope('generator'):\n",
        "            self.g_embeddings = tf.Variable(self.params[0])\n",
        "            self.g_params.append(self.g_embeddings)\n",
        "            self.g_recurrent_unit = self.create_recurrent_unit(self.g_params)  # maps h_tm1 to h_t for generator\n",
        "            self.g_output_unit = self.create_output_unit(self.g_params)  # maps h_t to o_t (output token logits)\n",
        "\n",
        "        # placeholder definition\n",
        "        self.x = tf.placeholder(tf.int32, shape=[self.batch_size, self.sequence_length]) # sequence of tokens generated by generator\n",
        "\n",
        "        # processed for batch\n",
        "        with tf.device(\"/cpu:0\"):\n",
        "            self.processed_x = tf.transpose(tf.nn.embedding_lookup(self.g_embeddings, self.x), perm=[1, 0, 2])  # seq_length x batch_size x emb_dim\n",
        "\n",
        "        # initial states\n",
        "        self.h0 = tf.zeros([self.batch_size, self.hidden_dim])\n",
        "        self.h0 = tf.stack([self.h0, self.h0])\n",
        "\n",
        "        # generator on initial randomness\n",
        "        gen_o = tensor_array_ops.TensorArray(dtype=tf.float32, size=self.sequence_length,\n",
        "                                             dynamic_size=False, infer_shape=True)\n",
        "        gen_x = tensor_array_ops.TensorArray(dtype=tf.int32, size=self.sequence_length,\n",
        "                                             dynamic_size=False, infer_shape=True)\n",
        "\n",
        "        def _g_recurrence(i, x_t, h_tm1, gen_o, gen_x):\n",
        "            h_t = self.g_recurrent_unit(x_t, h_tm1)  # hidden_memory_tuple\n",
        "            o_t = self.g_output_unit(h_t)  # batch x vocab , logits not prob\n",
        "            log_prob = tf.log(tf.nn.softmax(o_t))\n",
        "            next_token = tf.cast(tf.reshape(tf.multinomial(log_prob, 1), [self.batch_size]), tf.int32)\n",
        "            x_tp1 = tf.nn.embedding_lookup(self.g_embeddings, next_token)  # batch x emb_dim\n",
        "            gen_o = gen_o.write(i, tf.reduce_sum(tf.multiply(tf.one_hot(next_token, self.num_emb, 1.0, 0.0),\n",
        "                                                             tf.nn.softmax(o_t)), 1))  # [batch_size] , prob\n",
        "            gen_x = gen_x.write(i, next_token)  # indices, batch_size\n",
        "            return i + 1, x_tp1, h_t, gen_o, gen_x\n",
        "\n",
        "        _, _, _, self.gen_o, self.gen_x = control_flow_ops.while_loop(\n",
        "            cond=lambda i, _1, _2, _3, _4: i < self.sequence_length,\n",
        "            body=_g_recurrence,\n",
        "            loop_vars=(tf.constant(0, dtype=tf.int32),\n",
        "                       tf.nn.embedding_lookup(self.g_embeddings, self.start_token), self.h0, gen_o, gen_x)\n",
        "            )\n",
        "\n",
        "        self.gen_x = self.gen_x.stack()  # seq_length x batch_size\n",
        "        self.gen_x = tf.transpose(self.gen_x, perm=[1, 0])  # batch_size x seq_length\n",
        "\n",
        "        # supervised pretraining for generator\n",
        "        g_predictions = tensor_array_ops.TensorArray(\n",
        "            dtype=tf.float32, size=self.sequence_length,\n",
        "            dynamic_size=False, infer_shape=True)\n",
        "\n",
        "        ta_emb_x = tensor_array_ops.TensorArray(\n",
        "            dtype=tf.float32, size=self.sequence_length)\n",
        "        ta_emb_x = ta_emb_x.unstack(self.processed_x)\n",
        "\n",
        "        def _pretrain_recurrence(i, x_t, h_tm1, g_predictions):\n",
        "            h_t = self.g_recurrent_unit(x_t, h_tm1)\n",
        "            o_t = self.g_output_unit(h_t)\n",
        "            g_predictions = g_predictions.write(i, tf.nn.softmax(o_t))  # batch x vocab_size\n",
        "            x_tp1 = ta_emb_x.read(i)\n",
        "            return i + 1, x_tp1, h_t, g_predictions\n",
        "\n",
        "        _, _, _, self.g_predictions = control_flow_ops.while_loop(\n",
        "            cond=lambda i, _1, _2, _3: i < self.sequence_length,\n",
        "            body=_pretrain_recurrence,\n",
        "            loop_vars=(tf.constant(0, dtype=tf.int32),\n",
        "                       tf.nn.embedding_lookup(self.g_embeddings, self.start_token),\n",
        "                       self.h0, g_predictions))\n",
        "\n",
        "        self.g_predictions = tf.transpose(\n",
        "            self.g_predictions.stack(), perm=[1, 0, 2])  # batch_size x seq_length x vocab_size\n",
        "\n",
        "        # pretraining loss\n",
        "        self.pretrain_loss = -tf.reduce_sum(\n",
        "            tf.one_hot(tf.to_int32(tf.reshape(self.x, [-1])), self.num_emb, 1.0, 0.0) * tf.log(\n",
        "                tf.reshape(self.g_predictions, [-1, self.num_emb]))) / (self.sequence_length * self.batch_size)\n",
        "\n",
        "        self.out_loss = tf.reduce_sum(\n",
        "            tf.reshape(\n",
        "                -tf.reduce_sum(\n",
        "                    tf.one_hot(tf.to_int32(tf.reshape(self.x, [-1])), self.num_emb, 1.0, 0.0) * tf.log(\n",
        "                        tf.reshape(self.g_predictions, [-1, self.num_emb])), 1\n",
        "                ), [-1, self.sequence_length]\n",
        "            ), 1\n",
        "        )  # batch_size\n",
        "\n",
        "    def generate(self, session):\n",
        "        # h0 = np.random.normal(size=self.hidden_dim)\n",
        "        outputs = session.run(self.gen_x)\n",
        "        return outputs\n",
        "\n",
        "    def init_matrix(self, shape):\n",
        "        return tf.random_normal(shape, stddev=1.0)\n",
        "\n",
        "    def create_recurrent_unit(self, params):\n",
        "        # Weights and Bias for input and hidden tensor\n",
        "        self.Wi = tf.Variable(self.params[1])\n",
        "        self.Ui = tf.Variable(self.params[2])\n",
        "        self.bi = tf.Variable(self.params[3])\n",
        "\n",
        "        self.Wf = tf.Variable(self.params[4])\n",
        "        self.Uf = tf.Variable(self.params[5])\n",
        "        self.bf = tf.Variable(self.params[6])\n",
        "\n",
        "        self.Wog = tf.Variable(self.params[7])\n",
        "        self.Uog = tf.Variable(self.params[8])\n",
        "        self.bog = tf.Variable(self.params[9])\n",
        "\n",
        "        self.Wc = tf.Variable(self.params[10])\n",
        "        self.Uc = tf.Variable(self.params[11])\n",
        "        self.bc = tf.Variable(self.params[12])\n",
        "        params.extend([\n",
        "            self.Wi, self.Ui, self.bi,\n",
        "            self.Wf, self.Uf, self.bf,\n",
        "            self.Wog, self.Uog, self.bog,\n",
        "            self.Wc, self.Uc, self.bc])\n",
        "\n",
        "        def unit(x, hidden_memory_tm1):\n",
        "            previous_hidden_state, c_prev = tf.unstack(hidden_memory_tm1)\n",
        "\n",
        "            # Input Gate\n",
        "            i = tf.sigmoid(\n",
        "                tf.matmul(x, self.Wi) +\n",
        "                tf.matmul(previous_hidden_state, self.Ui) + self.bi\n",
        "            )\n",
        "\n",
        "            # Forget Gate\n",
        "            f = tf.sigmoid(\n",
        "                tf.matmul(x, self.Wf) +\n",
        "                tf.matmul(previous_hidden_state, self.Uf) + self.bf\n",
        "            )\n",
        "\n",
        "            # Output Gate\n",
        "            o = tf.sigmoid(\n",
        "                tf.matmul(x, self.Wog) +\n",
        "                tf.matmul(previous_hidden_state, self.Uog) + self.bog\n",
        "            )\n",
        "\n",
        "            # New Memory Cell\n",
        "            c_ = tf.nn.tanh(\n",
        "                tf.matmul(x, self.Wc) +\n",
        "                tf.matmul(previous_hidden_state, self.Uc) + self.bc\n",
        "            )\n",
        "\n",
        "            # Final Memory cell\n",
        "            c = f * c_prev + i * c_\n",
        "\n",
        "            # Current Hidden state\n",
        "            current_hidden_state = o * tf.nn.tanh(c)\n",
        "\n",
        "            return tf.stack([current_hidden_state, c])\n",
        "\n",
        "        return unit\n",
        "\n",
        "    def create_output_unit(self, params):\n",
        "        self.Wo = tf.Variable(self.params[13])\n",
        "        self.bo = tf.Variable(self.params[14])\n",
        "        params.extend([self.Wo, self.bo])\n",
        "\n",
        "        def unit(hidden_memory_tuple):\n",
        "            hidden_state, c_prev = tf.unstack(hidden_memory_tuple)\n",
        "            # hidden_state : batch x hidden_dim\n",
        "            logits = tf.matmul(hidden_state, self.Wo) + self.bo\n",
        "            # output = tf.nn.softmax(logits)\n",
        "            return logits\n",
        "\n",
        "        return unit\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LAppMTZWQdwK",
        "colab_type": "code",
        "outputId": "856e47e1-5cfd-43bb-86df-9423edc94a19",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive \n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tJEsOKkIQZIW",
        "colab_type": "code",
        "outputId": "9da88c20-e4db-49f2-8f9f-530332ef63e7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 894
        }
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import random\n",
        "#from dataloader import Gen_Data_loader, Dis_dataloader\n",
        "#from generator import Generator\n",
        "#from discriminator import Discriminator\n",
        "#from rollout import ROLLOUT\n",
        "#from target_lstm import TARGET_LSTM\n",
        "import pickle\n",
        "\n",
        "#########################################################################################\n",
        "#  Generator  Hyper-parameters\n",
        "######################################################################################\n",
        "EMB_DIM = 32 # embedding dimension\n",
        "HIDDEN_DIM = 32 # hidden state dimension of lstm cell\n",
        "SEQ_LENGTH = 20 # sequence length\n",
        "START_TOKEN = 0\n",
        "PRE_EPOCH_NUM = 120 # supervise (maximum likelihood estimation) epochs\n",
        "SEED = 88\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "#########################################################################################\n",
        "#  Discriminator  Hyper-parameters\n",
        "#########################################################################################\n",
        "dis_embedding_dim = 64\n",
        "dis_filter_sizes = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 15, 20]\n",
        "dis_num_filters = [100, 200, 200, 200, 200, 100, 100, 100, 100, 100, 160, 160]\n",
        "dis_dropout_keep_prob = 0.75\n",
        "dis_l2_reg_lambda = 0.2\n",
        "dis_batch_size = 64\n",
        "\n",
        "#########################################################################################\n",
        "#  Basic Training Parameters\n",
        "#########################################################################################\n",
        "TOTAL_BATCH = 200\n",
        "positive_file = 'drive/My Drive/save/real_data.txt'\n",
        "negative_file = 'drive/My Drive/save/generator_sample.txt'\n",
        "eval_file = 'drive/My Drive/save/eval_file.txt'\n",
        "generated_num = 10000\n",
        "\n",
        "\n",
        "def generate_samples(sess, trainable_model, batch_size, generated_num, output_file):\n",
        "    # Generate Samples\n",
        "    generated_samples = []\n",
        "    for _ in range(int(generated_num / batch_size)):\n",
        "        generated_samples.extend(trainable_model.generate(sess))\n",
        "\n",
        "    with open(output_file, 'w') as fout:\n",
        "        for poem in generated_samples:\n",
        "            buffer = ' '.join([str(x) for x in poem]) + '\\n'\n",
        "            fout.write(buffer)\n",
        "\n",
        "\n",
        "def target_loss(sess, target_lstm, data_loader):\n",
        "    # target_loss means the oracle negative log-likelihood tested with the oracle model \"target_lstm\"\n",
        "    # For more details, please see the Section 4 in https://arxiv.org/abs/1609.05473\n",
        "    nll = []\n",
        "    data_loader.reset_pointer()\n",
        "\n",
        "    for it in range(data_loader.num_batch):\n",
        "        batch = data_loader.next_batch()\n",
        "        g_loss = sess.run(target_lstm.pretrain_loss, {target_lstm.x: batch})\n",
        "        nll.append(g_loss)\n",
        "\n",
        "    return np.mean(nll)\n",
        "\n",
        "\n",
        "def pre_train_epoch(sess, trainable_model, data_loader):\n",
        "    # Pre-train the generator using MLE for one epoch\n",
        "    supervised_g_losses = []\n",
        "    data_loader.reset_pointer()\n",
        "\n",
        "    for it in range(data_loader.num_batch):\n",
        "        batch = data_loader.next_batch()\n",
        "        _, g_loss = trainable_model.pretrain_step(sess, batch)\n",
        "        supervised_g_losses.append(g_loss)\n",
        "\n",
        "    return np.mean(supervised_g_losses)\n",
        "\n",
        "\n",
        "def main():\n",
        "    random.seed(SEED)\n",
        "    np.random.seed(SEED)\n",
        "    assert START_TOKEN == 0\n",
        "\n",
        "    gen_data_loader = Gen_Data_loader(BATCH_SIZE)\n",
        "    likelihood_data_loader = Gen_Data_loader(BATCH_SIZE) # For testing\n",
        "    vocab_size = 5000\n",
        "    dis_data_loader = Dis_dataloader(BATCH_SIZE)\n",
        "\n",
        "    generator = Generator(vocab_size, BATCH_SIZE, EMB_DIM, HIDDEN_DIM, SEQ_LENGTH, START_TOKEN)\n",
        "    target_params = pickle.load(open('drive/My Drive/save/target_params.pkl', 'rb'), encoding='latin1')\n",
        "    target_lstm = TARGET_LSTM(vocab_size, BATCH_SIZE, EMB_DIM, HIDDEN_DIM, SEQ_LENGTH, START_TOKEN, target_params) # The oracle model\n",
        "\n",
        "    discriminator = Discriminator(sequence_length=20, num_classes=2, vocab_size=vocab_size, embedding_size=dis_embedding_dim, \n",
        "                                filter_sizes=dis_filter_sizes, num_filters=dis_num_filters, l2_reg_lambda=dis_l2_reg_lambda)\n",
        "\n",
        "    config = tf.ConfigProto()\n",
        "    config.gpu_options.allow_growth = True\n",
        "    sess = tf.Session(config=config)\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "\n",
        "    # First, use the oracle model to provide the positive examples, which are sampled from the oracle data distribution\n",
        "    generate_samples(sess, target_lstm, BATCH_SIZE, generated_num, positive_file)\n",
        "    gen_data_loader.create_batches(positive_file)\n",
        "\n",
        "    log = open('drive/My Drive/save/experiment-log.txt', 'w')\n",
        "    #  pre-train generator\n",
        "    print ('Start pre-training...')\n",
        "    log.write('pre-training...\\n')\n",
        "    for epoch in range(PRE_EPOCH_NUM):\n",
        "        loss = pre_train_epoch(sess, generator, gen_data_loader)\n",
        "        if epoch % 5 == 0:\n",
        "            generate_samples(sess, generator, BATCH_SIZE, generated_num, eval_file)\n",
        "            likelihood_data_loader.create_batches(eval_file)\n",
        "            test_loss = target_loss(sess, target_lstm, likelihood_data_loader)\n",
        "            print ('pre-train epoch ', epoch, 'test_loss ', test_loss)\n",
        "            buffer = 'epoch:\\t'+ str(epoch) + '\\tnll:\\t' + str(test_loss) + '\\n'\n",
        "            log.write(buffer)\n",
        "\n",
        "    print ('Start pre-training discriminator...')\n",
        "    # Train 3 epoch on the generated data and do this for 50 times\n",
        "    for _ in range(50):\n",
        "        generate_samples(sess, generator, BATCH_SIZE, generated_num, negative_file)\n",
        "        dis_data_loader.load_train_data(positive_file, negative_file)\n",
        "        for _ in range(3):\n",
        "            dis_data_loader.reset_pointer()\n",
        "            for it in range(dis_data_loader.num_batch):\n",
        "                x_batch, y_batch = dis_data_loader.next_batch()\n",
        "                feed = {\n",
        "                    discriminator.input_x: x_batch,\n",
        "                    discriminator.input_y: y_batch,\n",
        "                    discriminator.dropout_keep_prob: dis_dropout_keep_prob\n",
        "                }\n",
        "                _ = sess.run(discriminator.train_op, feed)\n",
        "\n",
        "    rollout = ROLLOUT(generator, 0.8)\n",
        "\n",
        "    print ('#########################################################################')\n",
        "    print ('Start Adversarial Training...')\n",
        "    log.write('adversarial training...\\n')\n",
        "    for total_batch in range(TOTAL_BATCH):\n",
        "        # Train the generator for one step\n",
        "        for it in range(1):\n",
        "            samples = generator.generate(sess)\n",
        "            rewards = rollout.get_reward(sess, samples, 16, discriminator)\n",
        "            feed = {generator.x: samples, generator.rewards: rewards}\n",
        "            _ = sess.run(generator.g_updates, feed_dict=feed)\n",
        "\n",
        "        # Test\n",
        "        if total_batch % 5 == 0 or total_batch == TOTAL_BATCH - 1:\n",
        "            generate_samples(sess, generator, BATCH_SIZE, generated_num, eval_file)\n",
        "            likelihood_data_loader.create_batches(eval_file)\n",
        "            test_loss = target_loss(sess, target_lstm, likelihood_data_loader)\n",
        "            buffer = 'epoch:\\t' + str(total_batch) + '\\tnll:\\t' + str(test_loss) + '\\n'\n",
        "            print ('total_batch: ', total_batch, 'test_loss: ', test_loss)\n",
        "            log.write(buffer)\n",
        "\n",
        "        # Update roll-out parameters\n",
        "        rollout.update_params()\n",
        "\n",
        "        # Train the discriminator\n",
        "        for _ in range(5):\n",
        "            generate_samples(sess, generator, BATCH_SIZE, generated_num, negative_file)\n",
        "            dis_data_loader.load_train_data(positive_file, negative_file)\n",
        "\n",
        "            for _ in range(3):\n",
        "                dis_data_loader.reset_pointer()\n",
        "                for it in range(dis_data_loader.num_batch):\n",
        "                    x_batch, y_batch = dis_data_loader.next_batch()\n",
        "                    feed = {\n",
        "                        discriminator.input_x: x_batch,\n",
        "                        discriminator.input_y: y_batch,\n",
        "                        discriminator.dropout_keep_prob: dis_dropout_keep_prob\n",
        "                    }\n",
        "                    _ = sess.run(discriminator.train_op, feed)\n",
        "\n",
        "    log.close()\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-2-25ef1897c222>:51: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.random.categorical` instead.\n",
            "WARNING:tensorflow:From <ipython-input-2-25ef1897c222>:94: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From <ipython-input-3-f33e5a9ddc9e>:115: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From <ipython-input-3-f33e5a9ddc9e>:129: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "\n",
            "Future major versions of TensorFlow will allow gradients to flow\n",
            "into the labels input on backprop by default.\n",
            "\n",
            "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
            "\n",
            "Start pre-training...\n",
            "pre-train epoch  0 test_loss  10.205985\n",
            "pre-train epoch  5 test_loss  9.516482\n",
            "pre-train epoch  10 test_loss  9.323244\n",
            "pre-train epoch  15 test_loss  9.210914\n",
            "pre-train epoch  20 test_loss  9.207461\n",
            "pre-train epoch  25 test_loss  9.172977\n",
            "pre-train epoch  30 test_loss  9.150039\n",
            "pre-train epoch  35 test_loss  9.137647\n",
            "pre-train epoch  40 test_loss  9.129892\n",
            "pre-train epoch  45 test_loss  9.11705\n",
            "pre-train epoch  50 test_loss  9.099761\n",
            "pre-train epoch  55 test_loss  9.098989\n",
            "pre-train epoch  60 test_loss  9.098421\n",
            "pre-train epoch  65 test_loss  9.119983\n",
            "pre-train epoch  70 test_loss  9.113192\n",
            "pre-train epoch  75 test_loss  9.118372\n",
            "pre-train epoch  80 test_loss  9.09877\n",
            "pre-train epoch  85 test_loss  9.100428\n",
            "pre-train epoch  90 test_loss  9.122629\n",
            "pre-train epoch  95 test_loss  9.100176\n",
            "pre-train epoch  100 test_loss  9.100617\n",
            "pre-train epoch  105 test_loss  9.103076\n",
            "pre-train epoch  110 test_loss  9.103846\n",
            "pre-train epoch  115 test_loss  9.111394\n",
            "Start pre-training discriminator...\n",
            "#########################################################################\n",
            "Start Adversarial Training...\n",
            "total_batch:  0 test_loss:  9.099348\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uU9AQdQAkuCm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}